{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library in python\n",
    "from Preprocessing import preprocess\n",
    "import glob\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "import csv\n",
    "from keras.models import Sequential,Model\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input,Dense,Conv1D,Activation,BatchNormalization,GlobalMaxPooling1D,Concatenate,Flatten,Dropout,Embedding\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix,accuracy_score\n",
    "import plotly.graph_objects as go\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocess files\n",
    "preprocessing = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data to generate the data\n",
    "def generateWordList(fileData): # Generate list of preprocessed words from file\n",
    "    finalSet = preprocessing.checkEmail(fileData) # Check emails in the file and add them to wordlist\n",
    "    fileData = preprocessing.removeEmail(fileData) # Remove emails \n",
    "    finalSet = finalSet + preprocessing.checkWebsite(fileData) # Check for website and decimal number and add them to wordlist\n",
    "    fileData = preprocessing.removeWebsite(fileData) # Remove webiste and decimal numbers \n",
    "    fileData = preprocessing.contractionsExpand(fileData) # Expand the contracted word\n",
    "    fileData = preprocessing.caseChange(fileData) # Change case of file to lower case\n",
    "    fileData = preprocessing.removePunctuations(fileData) # Remove punctutation \n",
    "    wordList = preprocessing.wordSeperator(fileData) # Seperate words by space\n",
    "    for word in wordList: # For each words in word list\n",
    "        if word.isdecimal(): # If number is present expand it \n",
    "            finalSet += preprocessing.changeNumber(word) # Generate number form of the number and add it to final list \n",
    "        else: \n",
    "            finalSet.append(preprocessing.lemmatizeWord(word)) # If it is word apply lemmatization\n",
    "    return finalSet # Return the fnal word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData = open(\"Annotated-Data.txt\",encoding=\"utf8\").read() # Load annotation files\n",
    "count = 0 # Count the file -- not used now\n",
    "documents = [] # store the documents\n",
    "y = [] # Load the answer vector\n",
    "for i in fileData.split(\"\\n\"): # Split data from the annotation file\n",
    "    splitData = i.split(\"\\t\") # Split annotation and article text\n",
    "    if len(splitData)>1: # If annotation file len > 1 that is some value is presents\n",
    "        y.append(int(splitData[0])) # Load final annotation \n",
    "        documents.append(\" \".join(generateWordList(\" \".join(splitData[1:])))) # Load preprocessed annotation\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"***Semeval Data***\"\"\"\n",
    "# fileData = open(\"SemEval-2019.txt\",encoding=\"utf8\").read()\n",
    "# count = 0 \n",
    "# documents = []\n",
    "# y = []\n",
    "# for i in fileData.split(\"\\n\"):\n",
    "#     splitData = i.split(\" \")\n",
    "#     if len(splitData)>1:\n",
    "#         y.append(int(splitData[1]))\n",
    "#         documents.append(\" \".join(generateWordList(\" \".join(splitData[2:]))))\n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 1000\n",
    "batch_size = 32\n",
    "embedding_dims = 500\n",
    "filters = 250\n",
    "kernel_size = [2,3,4,5,6]\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldy = y.copy()\n",
    "encoder = LabelEncoder() # Create a label encoder\n",
    "encoder.fit(y) # Fit label encoder\n",
    "encodedY = encoder.transform(y) # Transform label \n",
    "y = np_utils.to_categorical(encodedY) # Get categorial label here 1X3 as 3 labels given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(documents,y, test_size=0.20) # Split train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features) # Create tokenizer \n",
    "tokenizer.fit_on_texts(x_train) # Fit train data to tokenizer\n",
    "x_train = tokenizer.texts_to_sequences(x_train) # train_x data with tokeinzer output\n",
    "x_test = tokenizer.texts_to_sequences(x_test) # test_x data with tokeinzer output\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen) # Pad sequence so that length of document remains same -- remove extra data\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)  # Pad sequence so that length of document remains same -- remove extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = Input(shape=(maxlen,)) # Create input layer\n",
    "embedding = Embedding(max_features,embedding_dims,input_length=maxlen)(inputLayer) # Create embedding layer to provide embedding to text\n",
    "dropoutLayer = Dropout(0.2)(embedding) # Add dropout layer\n",
    "\n",
    "convolution1 = Conv1D(filters,kernel_size[0],padding='valid',activation='relu',strides=1)(dropoutLayer) # Add convolution layer 1\n",
    "branchNormaltize1 = BatchNormalization(momentum=0.5)(convolution1) # Batch normalize to provide better accuracy and fast processing\n",
    "activation1 = Activation('relu')(branchNormaltize1) # Add relu activation layer to convolution 1\n",
    "maxPooling1 = GlobalMaxPooling1D()(activation1) # Apply max polling to layer 1\n",
    "\n",
    "convolution2 = Conv1D(filters,kernel_size[1],padding='valid',activation='relu',strides=1)(dropoutLayer) # Add convolution layer 2\n",
    "branchNormaltize2 = BatchNormalization(momentum=0.5)(convolution2) # Batch normalize to provide better accuracy and fast processing\n",
    "activation2 = Activation('relu')(branchNormaltize2) # Add relu activation layer to convolution 2\n",
    "maxPooling2 = GlobalMaxPooling1D()(activation2) # Apply max polling to layer 2\n",
    "\n",
    "convolution3 = Conv1D(filters,kernel_size[2],padding='valid',activation='relu',strides=1)(dropoutLayer) # Add convolution layer 3\n",
    "branchNormaltize3 = BatchNormalization(momentum=0.5)(convolution3) # Batch normalize to provide better accuracy and fast processing\n",
    "activation3 = Activation('relu')(branchNormaltize3) # Add relu activation layer to convolution 3\n",
    "maxPooling3 = GlobalMaxPooling1D()(activation3) # Apply max polling to layer 3\n",
    "\n",
    "convolution4 = Conv1D(filters,kernel_size[3],padding='valid',activation='relu',strides=1)(dropoutLayer) # Add convolution layer 4\n",
    "branchNormaltize4 = BatchNormalization(momentum=0.5)(convolution4) # Batch normalize to provide better accuracy and fast processing\n",
    "activation4 = Activation('relu')(branchNormaltize4) # Add relu activation layer to convolution 4\n",
    "maxPooling4 = GlobalMaxPooling1D()(activation4) # Apply max polling to layer 4\n",
    "\n",
    "convolution5 = Conv1D(filters,kernel_size[4],padding='valid',activation='relu',strides=1)(dropoutLayer) # Add convolution layer 5\n",
    "branchNormaltize5 = BatchNormalization(momentum=0.5)(convolution5) # Batch normalize to provide better accuracy and fast processing\n",
    "activation5 = Activation('relu')(branchNormaltize5) # Add relu activation layer to convolution 5\n",
    "maxPooling5 = GlobalMaxPooling1D()(activation5) # Apply max polling to layer 5\n",
    "\n",
    "concatenationLayer = Concatenate()([maxPooling1, maxPooling2, maxPooling3,maxPooling4,maxPooling5]) # Concatenate layers\n",
    "\n",
    "dense1 = Dense(hidden_dims)(concatenationLayer) # Apply dense layer 1\n",
    "dropoutLayer2 = Dropout(0.2)(dense1) # Add dropout layer \n",
    "activationRelu = Activation('relu')(dropoutLayer2) # Add relu activation layer\n",
    "\n",
    "dense2 = Dense(classes)(activationRelu) # Apply dense layer 1\n",
    "activationSoftmax = Activation('softmax')(dense2) # Add softmax layer which will be the final output\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"Model.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model = Model(inputs=inputLayer, outputs=activationSoftmax) # Add input and output layer to create the network\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) # Compute with adamic adar and categorial cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning:\n",
      "\n",
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 724 samples, validate on 181 samples\n",
      "Epoch 1/20\n",
      "724/724 [==============================] - ETA: 4:06 - loss: 2.5972 - accuracy: 0.53 - ETA: 3:30 - loss: 9.2547 - accuracy: 0.40 - ETA: 3:17 - loss: 13.0425 - accuracy: 0.375 - ETA: 3:01 - loss: 13.8670 - accuracy: 0.343 - ETA: 2:46 - loss: 12.5166 - accuracy: 0.331 - ETA: 2:33 - loss: 11.3907 - accuracy: 0.328 - ETA: 2:22 - loss: 10.3026 - accuracy: 0.352 - ETA: 2:11 - loss: 9.5992 - accuracy: 0.371 - ETA: 2:01 - loss: 9.1564 - accuracy: 0.37 - ETA: 1:51 - loss: 8.6361 - accuracy: 0.38 - ETA: 1:42 - loss: 8.2580 - accuracy: 0.39 - ETA: 1:35 - loss: 8.1008 - accuracy: 0.38 - ETA: 1:28 - loss: 7.7835 - accuracy: 0.38 - ETA: 1:19 - loss: 7.5554 - accuracy: 0.38 - ETA: 1:10 - loss: 7.2956 - accuracy: 0.38 - ETA: 1:01 - loss: 7.0706 - accuracy: 0.38 - ETA: 51s - loss: 6.8570 - accuracy: 0.3805 - ETA: 42s - loss: 6.5978 - accuracy: 0.381 - ETA: 33s - loss: 6.3528 - accuracy: 0.376 - ETA: 24s - loss: 6.1434 - accuracy: 0.375 - ETA: 14s - loss: 6.0158 - accuracy: 0.367 - ETA: 5s - loss: 5.9131 - accuracy: 0.360 - 226s 312ms/step - loss: 5.8117 - accuracy: 0.3633 - val_loss: 2.0383 - val_accuracy: 0.2707\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.27072, saving model to Model.hdf5\n",
      "Epoch 2/20\n",
      "724/724 [==============================] - ETA: 3:16 - loss: 2.7915 - accuracy: 0.31 - ETA: 3:07 - loss: 2.2270 - accuracy: 0.39 - ETA: 2:56 - loss: 1.9097 - accuracy: 0.42 - ETA: 2:46 - loss: 1.6981 - accuracy: 0.47 - ETA: 2:36 - loss: 1.7465 - accuracy: 0.45 - ETA: 2:26 - loss: 1.6510 - accuracy: 0.48 - ETA: 2:16 - loss: 1.6533 - accuracy: 0.48 - ETA: 2:06 - loss: 1.5999 - accuracy: 0.47 - ETA: 1:57 - loss: 1.5288 - accuracy: 0.48 - ETA: 1:48 - loss: 1.4816 - accuracy: 0.49 - ETA: 1:40 - loss: 1.4437 - accuracy: 0.49 - ETA: 1:31 - loss: 1.3999 - accuracy: 0.49 - ETA: 1:22 - loss: 1.3674 - accuracy: 0.50 - ETA: 1:13 - loss: 1.3400 - accuracy: 0.50 - ETA: 1:05 - loss: 1.3067 - accuracy: 0.51 - ETA: 56s - loss: 1.2754 - accuracy: 0.5215 - ETA: 47s - loss: 1.2511 - accuracy: 0.523 - ETA: 39s - loss: 1.2304 - accuracy: 0.527 - ETA: 30s - loss: 1.2109 - accuracy: 0.536 - ETA: 22s - loss: 1.1816 - accuracy: 0.546 - ETA: 13s - loss: 1.1675 - accuracy: 0.543 - ETA: 5s - loss: 1.1413 - accuracy: 0.556 - 206s 284ms/step - loss: 1.1332 - accuracy: 0.5566 - val_loss: 1.0160 - val_accuracy: 0.5138\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.27072 to 0.51381, saving model to Model.hdf5\n",
      "Epoch 3/20\n",
      "724/724 [==============================] - ETA: 3:00 - loss: 0.6676 - accuracy: 0.75 - ETA: 2:51 - loss: 0.6307 - accuracy: 0.75 - ETA: 2:44 - loss: 0.6193 - accuracy: 0.78 - ETA: 2:35 - loss: 0.6146 - accuracy: 0.77 - ETA: 2:27 - loss: 0.5692 - accuracy: 0.78 - ETA: 2:18 - loss: 0.5269 - accuracy: 0.81 - ETA: 2:10 - loss: 0.5114 - accuracy: 0.81 - ETA: 2:01 - loss: 0.5130 - accuracy: 0.82 - ETA: 1:54 - loss: 0.5075 - accuracy: 0.82 - ETA: 1:45 - loss: 0.5146 - accuracy: 0.82 - ETA: 1:37 - loss: 0.5103 - accuracy: 0.82 - ETA: 1:28 - loss: 0.5143 - accuracy: 0.82 - ETA: 1:20 - loss: 0.5179 - accuracy: 0.81 - ETA: 1:12 - loss: 0.5158 - accuracy: 0.82 - ETA: 1:03 - loss: 0.5116 - accuracy: 0.82 - ETA: 55s - loss: 0.5104 - accuracy: 0.8262 - ETA: 47s - loss: 0.5001 - accuracy: 0.830 - ETA: 38s - loss: 0.4993 - accuracy: 0.833 - ETA: 30s - loss: 0.4908 - accuracy: 0.837 - ETA: 21s - loss: 0.4875 - accuracy: 0.835 - ETA: 13s - loss: 0.4857 - accuracy: 0.837 - ETA: 5s - loss: 0.4792 - accuracy: 0.840 - 203s 281ms/step - loss: 0.4805 - accuracy: 0.8384 - val_loss: 0.9832 - val_accuracy: 0.5470\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.51381 to 0.54696, saving model to Model.hdf5\n",
      "Epoch 4/20\n",
      "724/724 [==============================] - ETA: 2:57 - loss: 0.2939 - accuracy: 0.96 - ETA: 2:51 - loss: 0.2879 - accuracy: 0.95 - ETA: 2:42 - loss: 0.2980 - accuracy: 0.93 - ETA: 2:34 - loss: 0.3364 - accuracy: 0.90 - ETA: 2:26 - loss: 0.3207 - accuracy: 0.90 - ETA: 2:18 - loss: 0.2977 - accuracy: 0.91 - ETA: 2:09 - loss: 0.3014 - accuracy: 0.91 - ETA: 2:01 - loss: 0.3035 - accuracy: 0.90 - ETA: 1:53 - loss: 0.3090 - accuracy: 0.90 - ETA: 1:44 - loss: 0.3042 - accuracy: 0.91 - ETA: 1:36 - loss: 0.3062 - accuracy: 0.90 - ETA: 1:28 - loss: 0.2999 - accuracy: 0.91 - ETA: 1:19 - loss: 0.2988 - accuracy: 0.92 - ETA: 1:11 - loss: 0.2978 - accuracy: 0.91 - ETA: 1:03 - loss: 0.2936 - accuracy: 0.92 - ETA: 55s - loss: 0.2964 - accuracy: 0.9199 - ETA: 46s - loss: 0.2958 - accuracy: 0.919 - ETA: 38s - loss: 0.2977 - accuracy: 0.920 - ETA: 30s - loss: 0.2977 - accuracy: 0.919 - ETA: 21s - loss: 0.2969 - accuracy: 0.920 - ETA: 13s - loss: 0.3038 - accuracy: 0.916 - ETA: 5s - loss: 0.2990 - accuracy: 0.919 - 205s 283ms/step - loss: 0.2960 - accuracy: 0.9199 - val_loss: 1.0096 - val_accuracy: 0.5249\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.54696\n",
      "Epoch 5/20\n",
      "724/724 [==============================] - ETA: 3:34 - loss: 0.1169 - accuracy: 1.00 - ETA: 3:23 - loss: 0.1433 - accuracy: 1.00 - ETA: 3:09 - loss: 0.1516 - accuracy: 0.98 - ETA: 2:57 - loss: 0.1554 - accuracy: 0.98 - ETA: 2:47 - loss: 0.1535 - accuracy: 0.98 - ETA: 2:37 - loss: 0.1480 - accuracy: 0.98 - ETA: 2:29 - loss: 0.1569 - accuracy: 0.98 - ETA: 2:19 - loss: 0.1670 - accuracy: 0.98 - ETA: 2:10 - loss: 0.1818 - accuracy: 0.96 - ETA: 2:02 - loss: 0.1872 - accuracy: 0.96 - ETA: 1:52 - loss: 0.1960 - accuracy: 0.95 - ETA: 1:41 - loss: 0.1898 - accuracy: 0.95 - ETA: 1:30 - loss: 0.1884 - accuracy: 0.95 - ETA: 1:20 - loss: 0.1868 - accuracy: 0.95 - ETA: 1:10 - loss: 0.1866 - accuracy: 0.95 - ETA: 1:01 - loss: 0.1861 - accuracy: 0.95 - ETA: 51s - loss: 0.1845 - accuracy: 0.9577 - ETA: 42s - loss: 0.1808 - accuracy: 0.960 - ETA: 32s - loss: 0.1763 - accuracy: 0.962 - ETA: 23s - loss: 0.1763 - accuracy: 0.959 - ETA: 14s - loss: 0.1791 - accuracy: 0.956 - ETA: 5s - loss: 0.1786 - accuracy: 0.957 - 217s 299ms/step - loss: 0.1771 - accuracy: 0.9586 - val_loss: 1.0462 - val_accuracy: 0.5414\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.54696\n",
      "Epoch 6/20\n",
      "724/724 [==============================] - ETA: 3:00 - loss: 0.1155 - accuracy: 1.00 - ETA: 2:53 - loss: 0.0856 - accuracy: 1.00 - ETA: 2:47 - loss: 0.0902 - accuracy: 0.98 - ETA: 2:38 - loss: 0.1206 - accuracy: 0.96 - ETA: 2:34 - loss: 0.1167 - accuracy: 0.97 - ETA: 2:28 - loss: 0.1117 - accuracy: 0.97 - ETA: 2:19 - loss: 0.1015 - accuracy: 0.98 - ETA: 2:09 - loss: 0.1040 - accuracy: 0.98 - ETA: 2:00 - loss: 0.1025 - accuracy: 0.98 - ETA: 1:51 - loss: 0.0980 - accuracy: 0.98 - ETA: 1:42 - loss: 0.0959 - accuracy: 0.98 - ETA: 1:33 - loss: 0.0975 - accuracy: 0.98 - ETA: 1:24 - loss: 0.1016 - accuracy: 0.98 - ETA: 1:15 - loss: 0.0996 - accuracy: 0.98 - ETA: 1:06 - loss: 0.1014 - accuracy: 0.97 - ETA: 57s - loss: 0.0995 - accuracy: 0.9805 - ETA: 49s - loss: 0.0995 - accuracy: 0.979 - ETA: 40s - loss: 0.0993 - accuracy: 0.980 - ETA: 31s - loss: 0.0976 - accuracy: 0.981 - ETA: 22s - loss: 0.1011 - accuracy: 0.981 - ETA: 14s - loss: 0.1046 - accuracy: 0.980 - ETA: 5s - loss: 0.1133 - accuracy: 0.978 - 211s 291ms/step - loss: 0.1118 - accuracy: 0.9793 - val_loss: 1.0740 - val_accuracy: 0.5635\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.54696 to 0.56354, saving model to Model.hdf5\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724/724 [==============================] - ETA: 3:01 - loss: 0.1239 - accuracy: 0.93 - ETA: 2:55 - loss: 0.0856 - accuracy: 0.96 - ETA: 2:46 - loss: 0.0691 - accuracy: 0.97 - ETA: 2:38 - loss: 0.0700 - accuracy: 0.98 - ETA: 2:29 - loss: 0.0863 - accuracy: 0.98 - ETA: 2:21 - loss: 0.0862 - accuracy: 0.98 - ETA: 2:12 - loss: 0.0970 - accuracy: 0.98 - ETA: 2:04 - loss: 0.0965 - accuracy: 0.98 - ETA: 1:58 - loss: 0.0936 - accuracy: 0.98 - ETA: 1:51 - loss: 0.0924 - accuracy: 0.98 - ETA: 1:42 - loss: 0.0884 - accuracy: 0.98 - ETA: 1:33 - loss: 0.0868 - accuracy: 0.98 - ETA: 1:24 - loss: 0.0861 - accuracy: 0.98 - ETA: 1:15 - loss: 0.0913 - accuracy: 0.98 - ETA: 1:06 - loss: 0.0985 - accuracy: 0.98 - ETA: 57s - loss: 0.0956 - accuracy: 0.9863 - ETA: 49s - loss: 0.0945 - accuracy: 0.987 - ETA: 40s - loss: 0.0942 - accuracy: 0.987 - ETA: 31s - loss: 0.0917 - accuracy: 0.988 - ETA: 22s - loss: 0.0920 - accuracy: 0.987 - ETA: 14s - loss: 0.0986 - accuracy: 0.986 - ETA: 5s - loss: 0.0961 - accuracy: 0.987 - 212s 293ms/step - loss: 0.0954 - accuracy: 0.9876 - val_loss: 1.1560 - val_accuracy: 0.5691\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.56354 to 0.56906, saving model to Model.hdf5\n",
      "Epoch 8/20\n",
      "724/724 [==============================] - ETA: 2:58 - loss: 0.0947 - accuracy: 0.96 - ETA: 2:52 - loss: 0.0674 - accuracy: 0.98 - ETA: 2:45 - loss: 0.0717 - accuracy: 0.97 - ETA: 2:38 - loss: 0.0643 - accuracy: 0.98 - ETA: 2:34 - loss: 0.0639 - accuracy: 0.98 - ETA: 2:26 - loss: 0.0630 - accuracy: 0.98 - ETA: 2:17 - loss: 0.0666 - accuracy: 0.98 - ETA: 2:09 - loss: 0.0643 - accuracy: 0.98 - ETA: 2:02 - loss: 0.0619 - accuracy: 0.98 - ETA: 1:54 - loss: 0.0693 - accuracy: 0.98 - ETA: 1:45 - loss: 0.0662 - accuracy: 0.98 - ETA: 1:37 - loss: 0.0667 - accuracy: 0.98 - ETA: 1:28 - loss: 0.0753 - accuracy: 0.98 - ETA: 1:19 - loss: 0.0714 - accuracy: 0.98 - ETA: 1:11 - loss: 0.0758 - accuracy: 0.98 - ETA: 1:02 - loss: 0.0758 - accuracy: 0.98 - ETA: 52s - loss: 0.0762 - accuracy: 0.9798 - ETA: 43s - loss: 0.0739 - accuracy: 0.980 - ETA: 34s - loss: 0.0723 - accuracy: 0.981 - ETA: 24s - loss: 0.0704 - accuracy: 0.982 - ETA: 15s - loss: 0.0688 - accuracy: 0.983 - ETA: 5s - loss: 0.0697 - accuracy: 0.983 - 224s 310ms/step - loss: 0.0698 - accuracy: 0.9834 - val_loss: 1.1202 - val_accuracy: 0.5635\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.56906\n",
      "Epoch 9/20\n",
      "724/724 [==============================] - ETA: 3:01 - loss: 0.0330 - accuracy: 1.00 - ETA: 2:52 - loss: 0.0475 - accuracy: 1.00 - ETA: 2:44 - loss: 0.0501 - accuracy: 1.00 - ETA: 2:35 - loss: 0.0480 - accuracy: 1.00 - ETA: 2:27 - loss: 0.0434 - accuracy: 1.00 - ETA: 2:18 - loss: 0.0449 - accuracy: 0.99 - ETA: 2:10 - loss: 0.0439 - accuracy: 0.99 - ETA: 2:02 - loss: 0.0737 - accuracy: 0.98 - ETA: 1:53 - loss: 0.0692 - accuracy: 0.98 - ETA: 1:45 - loss: 0.0652 - accuracy: 0.99 - ETA: 1:37 - loss: 0.0653 - accuracy: 0.99 - ETA: 1:29 - loss: 0.0628 - accuracy: 0.99 - ETA: 1:21 - loss: 0.0603 - accuracy: 0.99 - ETA: 1:13 - loss: 0.0594 - accuracy: 0.99 - ETA: 1:05 - loss: 0.0570 - accuracy: 0.99 - ETA: 57s - loss: 0.0554 - accuracy: 0.9941 - ETA: 48s - loss: 0.0548 - accuracy: 0.994 - ETA: 40s - loss: 0.0569 - accuracy: 0.993 - ETA: 31s - loss: 0.0573 - accuracy: 0.993 - ETA: 22s - loss: 0.0560 - accuracy: 0.993 - ETA: 14s - loss: 0.0553 - accuracy: 0.994 - ETA: 5s - loss: 0.0553 - accuracy: 0.992 - 210s 290ms/step - loss: 0.0590 - accuracy: 0.9917 - val_loss: 1.1979 - val_accuracy: 0.5635\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.56906\n",
      "Epoch 10/20\n",
      "724/724 [==============================] - ETA: 3:03 - loss: 0.0475 - accuracy: 1.00 - ETA: 2:55 - loss: 0.0531 - accuracy: 0.98 - ETA: 2:47 - loss: 0.0454 - accuracy: 0.98 - ETA: 2:39 - loss: 0.0394 - accuracy: 0.99 - ETA: 2:30 - loss: 0.0389 - accuracy: 0.99 - ETA: 2:21 - loss: 0.0464 - accuracy: 0.98 - ETA: 2:13 - loss: 0.0455 - accuracy: 0.99 - ETA: 2:04 - loss: 0.0422 - accuracy: 0.99 - ETA: 1:56 - loss: 0.0405 - accuracy: 0.99 - ETA: 1:48 - loss: 0.0387 - accuracy: 0.99 - ETA: 1:40 - loss: 0.0513 - accuracy: 0.98 - ETA: 1:31 - loss: 0.0493 - accuracy: 0.98 - ETA: 1:22 - loss: 0.0483 - accuracy: 0.99 - ETA: 1:14 - loss: 0.0463 - accuracy: 0.99 - ETA: 1:06 - loss: 0.0442 - accuracy: 0.99 - ETA: 57s - loss: 0.0430 - accuracy: 0.9922 - ETA: 49s - loss: 0.0415 - accuracy: 0.992 - ETA: 40s - loss: 0.0403 - accuracy: 0.993 - ETA: 32s - loss: 0.0404 - accuracy: 0.993 - ETA: 23s - loss: 0.0408 - accuracy: 0.992 - ETA: 14s - loss: 0.0400 - accuracy: 0.992 - ETA: 5s - loss: 0.0391 - accuracy: 0.992 - 215s 297ms/step - loss: 0.0388 - accuracy: 0.9931 - val_loss: 1.1958 - val_accuracy: 0.5635\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.56906\n",
      "Epoch 11/20\n",
      "724/724 [==============================] - ETA: 3:01 - loss: 0.0132 - accuracy: 1.00 - ETA: 2:54 - loss: 0.0185 - accuracy: 1.00 - ETA: 2:47 - loss: 0.0202 - accuracy: 1.00 - ETA: 2:47 - loss: 0.0199 - accuracy: 1.00 - ETA: 2:44 - loss: 0.0200 - accuracy: 1.00 - ETA: 2:33 - loss: 0.0185 - accuracy: 1.00 - ETA: 2:22 - loss: 0.0339 - accuracy: 0.99 - ETA: 2:12 - loss: 0.0311 - accuracy: 0.99 - ETA: 2:02 - loss: 0.0336 - accuracy: 0.99 - ETA: 1:53 - loss: 0.0318 - accuracy: 0.99 - ETA: 1:45 - loss: 0.0382 - accuracy: 0.99 - ETA: 1:35 - loss: 0.0620 - accuracy: 0.98 - ETA: 1:26 - loss: 0.0592 - accuracy: 0.99 - ETA: 1:16 - loss: 0.0574 - accuracy: 0.99 - ETA: 1:07 - loss: 0.0558 - accuracy: 0.99 - ETA: 58s - loss: 0.0561 - accuracy: 0.9922 - ETA: 49s - loss: 0.0544 - accuracy: 0.992 - ETA: 40s - loss: 0.0524 - accuracy: 0.993 - ETA: 31s - loss: 0.0652 - accuracy: 0.991 - ETA: 23s - loss: 0.0631 - accuracy: 0.992 - ETA: 14s - loss: 0.0615 - accuracy: 0.992 - ETA: 5s - loss: 0.0592 - accuracy: 0.992 - 216s 298ms/step - loss: 0.0578 - accuracy: 0.9931 - val_loss: 1.3063 - val_accuracy: 0.4972\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.56906\n",
      "Epoch 12/20\n",
      "724/724 [==============================] - ETA: 3:01 - loss: 0.0591 - accuracy: 0.96 - ETA: 2:52 - loss: 0.0365 - accuracy: 0.98 - ETA: 2:44 - loss: 0.0329 - accuracy: 0.98 - ETA: 2:35 - loss: 0.0317 - accuracy: 0.99 - ETA: 2:27 - loss: 0.0283 - accuracy: 0.99 - ETA: 2:19 - loss: 0.0306 - accuracy: 0.99 - ETA: 2:10 - loss: 0.0289 - accuracy: 0.99 - ETA: 2:02 - loss: 0.0280 - accuracy: 0.99 - ETA: 1:54 - loss: 0.0277 - accuracy: 0.99 - ETA: 1:45 - loss: 0.0287 - accuracy: 0.99 - ETA: 1:37 - loss: 0.0296 - accuracy: 0.99 - ETA: 1:29 - loss: 0.0285 - accuracy: 0.99 - ETA: 1:20 - loss: 0.0272 - accuracy: 0.99 - ETA: 1:12 - loss: 0.0324 - accuracy: 0.99 - ETA: 1:03 - loss: 0.0319 - accuracy: 0.99 - ETA: 55s - loss: 0.0316 - accuracy: 0.9941 - ETA: 47s - loss: 0.0303 - accuracy: 0.994 - ETA: 38s - loss: 0.0297 - accuracy: 0.994 - ETA: 30s - loss: 0.0292 - accuracy: 0.995 - ETA: 22s - loss: 0.0281 - accuracy: 0.995 - ETA: 13s - loss: 0.0282 - accuracy: 0.995 - ETA: 5s - loss: 0.0284 - accuracy: 0.995 - 204s 281ms/step - loss: 0.0279 - accuracy: 0.9959 - val_loss: 1.2360 - val_accuracy: 0.5746\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.56906 to 0.57459, saving model to Model.hdf5\n",
      "Epoch 13/20\n",
      "724/724 [==============================] - ETA: 3:01 - loss: 0.0432 - accuracy: 0.96 - ETA: 2:54 - loss: 0.0284 - accuracy: 0.98 - ETA: 2:46 - loss: 0.0267 - accuracy: 0.98 - ETA: 2:40 - loss: 0.0241 - accuracy: 0.99 - ETA: 2:32 - loss: 0.0217 - accuracy: 0.99 - ETA: 2:22 - loss: 0.0196 - accuracy: 0.99 - ETA: 2:14 - loss: 0.0197 - accuracy: 0.99 - ETA: 2:05 - loss: 0.0206 - accuracy: 0.99 - ETA: 1:56 - loss: 0.0213 - accuracy: 0.99 - ETA: 1:47 - loss: 0.0295 - accuracy: 0.99 - ETA: 1:39 - loss: 0.0476 - accuracy: 0.98 - ETA: 1:30 - loss: 0.0455 - accuracy: 0.98 - ETA: 1:22 - loss: 0.0431 - accuracy: 0.99 - ETA: 1:14 - loss: 0.0409 - accuracy: 0.99 - ETA: 1:06 - loss: 0.0398 - accuracy: 0.99 - ETA: 57s - loss: 0.0383 - accuracy: 0.9922 - ETA: 49s - loss: 0.0371 - accuracy: 0.992 - ETA: 40s - loss: 0.0356 - accuracy: 0.993 - ETA: 31s - loss: 0.0343 - accuracy: 0.993 - ETA: 23s - loss: 0.0361 - accuracy: 0.992 - ETA: 14s - loss: 0.0350 - accuracy: 0.992 - ETA: 5s - loss: 0.0349 - accuracy: 0.992 - 218s 300ms/step - loss: 0.0343 - accuracy: 0.9931 - val_loss: 1.3546 - val_accuracy: 0.5083\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: val_accuracy did not improve from 0.57459\n",
      "Epoch 14/20\n",
      "724/724 [==============================] - ETA: 3:19 - loss: 0.0157 - accuracy: 1.00 - ETA: 3:10 - loss: 0.0131 - accuracy: 1.00 - ETA: 2:59 - loss: 0.0186 - accuracy: 1.00 - ETA: 2:50 - loss: 0.0202 - accuracy: 1.00 - ETA: 2:42 - loss: 0.0185 - accuracy: 1.00 - ETA: 2:33 - loss: 0.0170 - accuracy: 1.00 - ETA: 2:25 - loss: 0.0307 - accuracy: 0.99 - ETA: 2:15 - loss: 0.0278 - accuracy: 0.99 - ETA: 2:06 - loss: 0.0260 - accuracy: 0.99 - ETA: 1:57 - loss: 0.0250 - accuracy: 0.99 - ETA: 1:47 - loss: 0.0233 - accuracy: 0.99 - ETA: 1:38 - loss: 0.0220 - accuracy: 0.99 - ETA: 1:30 - loss: 0.0378 - accuracy: 0.99 - ETA: 1:21 - loss: 0.0367 - accuracy: 0.99 - ETA: 1:12 - loss: 0.0413 - accuracy: 0.99 - ETA: 1:03 - loss: 0.0413 - accuracy: 0.99 - ETA: 53s - loss: 0.0406 - accuracy: 0.9890 - ETA: 44s - loss: 0.0392 - accuracy: 0.989 - ETA: 34s - loss: 0.0376 - accuracy: 0.990 - ETA: 24s - loss: 0.0363 - accuracy: 0.990 - ETA: 15s - loss: 0.0354 - accuracy: 0.991 - ETA: 5s - loss: 0.0353 - accuracy: 0.991 - 231s 319ms/step - loss: 0.0344 - accuracy: 0.9917 - val_loss: 1.3531 - val_accuracy: 0.4972\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.57459\n",
      "Epoch 15/20\n",
      "724/724 [==============================] - ETA: 3:23 - loss: 0.0083 - accuracy: 1.00 - ETA: 3:14 - loss: 0.0570 - accuracy: 0.98 - ETA: 3:02 - loss: 0.0498 - accuracy: 0.98 - ETA: 2:52 - loss: 0.0404 - accuracy: 0.99 - ETA: 2:42 - loss: 0.0384 - accuracy: 0.99 - ETA: 2:32 - loss: 0.0342 - accuracy: 0.99 - ETA: 2:24 - loss: 0.0312 - accuracy: 0.99 - ETA: 2:15 - loss: 0.0299 - accuracy: 0.99 - ETA: 2:06 - loss: 0.0280 - accuracy: 0.99 - ETA: 1:58 - loss: 0.0268 - accuracy: 0.99 - ETA: 1:49 - loss: 0.0308 - accuracy: 0.99 - ETA: 1:39 - loss: 0.0289 - accuracy: 0.99 - ETA: 1:30 - loss: 0.0272 - accuracy: 0.99 - ETA: 1:21 - loss: 0.0279 - accuracy: 0.99 - ETA: 1:11 - loss: 0.0267 - accuracy: 0.99 - ETA: 1:02 - loss: 0.0261 - accuracy: 0.99 - ETA: 53s - loss: 0.0262 - accuracy: 0.9945 - ETA: 43s - loss: 0.0253 - accuracy: 0.994 - ETA: 34s - loss: 0.0259 - accuracy: 0.995 - ETA: 24s - loss: 0.0272 - accuracy: 0.993 - ETA: 15s - loss: 0.0265 - accuracy: 0.994 - ETA: 5s - loss: 0.0358 - accuracy: 0.992 - 230s 317ms/step - loss: 0.0350 - accuracy: 0.9931 - val_loss: 1.3234 - val_accuracy: 0.5193\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.57459\n",
      "Epoch 16/20\n",
      "724/724 [==============================] - ETA: 3:23 - loss: 0.0169 - accuracy: 1.00 - ETA: 3:09 - loss: 0.0706 - accuracy: 0.98 - ETA: 3:00 - loss: 0.0530 - accuracy: 0.98 - ETA: 2:59 - loss: 0.0477 - accuracy: 0.99 - ETA: 3:02 - loss: 0.0399 - accuracy: 0.99 - ETA: 2:58 - loss: 0.0347 - accuracy: 0.99 - ETA: 2:49 - loss: 0.0308 - accuracy: 0.99 - ETA: 2:39 - loss: 0.0283 - accuracy: 0.99 - ETA: 2:29 - loss: 0.0260 - accuracy: 0.99 - ETA: 2:19 - loss: 0.0238 - accuracy: 0.99 - ETA: 2:07 - loss: 0.0236 - accuracy: 0.99 - ETA: 1:55 - loss: 0.0226 - accuracy: 0.99 - ETA: 1:43 - loss: 0.0224 - accuracy: 0.99 - ETA: 1:32 - loss: 0.0219 - accuracy: 0.99 - ETA: 1:21 - loss: 0.0228 - accuracy: 0.99 - ETA: 1:10 - loss: 0.0243 - accuracy: 0.99 - ETA: 59s - loss: 0.0239 - accuracy: 0.9982 - ETA: 48s - loss: 0.0229 - accuracy: 0.998 - ETA: 37s - loss: 0.0268 - accuracy: 0.996 - ETA: 27s - loss: 0.0260 - accuracy: 0.996 - ETA: 16s - loss: 0.0253 - accuracy: 0.997 - ETA: 6s - loss: 0.0250 - accuracy: 0.997 - 246s 339ms/step - loss: 0.0254 - accuracy: 0.9972 - val_loss: 1.2626 - val_accuracy: 0.5525\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.57459\n",
      "Epoch 17/20\n",
      "724/724 [==============================] - ETA: 3:16 - loss: 0.0389 - accuracy: 1.00 - ETA: 3:06 - loss: 0.0489 - accuracy: 0.98 - ETA: 2:57 - loss: 0.0365 - accuracy: 0.98 - ETA: 2:47 - loss: 0.0286 - accuracy: 0.99 - ETA: 2:38 - loss: 0.0280 - accuracy: 0.99 - ETA: 2:29 - loss: 0.0254 - accuracy: 0.99 - ETA: 2:20 - loss: 0.0233 - accuracy: 0.99 - ETA: 2:10 - loss: 0.0227 - accuracy: 0.99 - ETA: 2:01 - loss: 0.0227 - accuracy: 0.99 - ETA: 1:53 - loss: 0.0241 - accuracy: 0.99 - ETA: 1:44 - loss: 0.0228 - accuracy: 0.99 - ETA: 1:35 - loss: 0.0218 - accuracy: 0.99 - ETA: 1:26 - loss: 0.0261 - accuracy: 0.99 - ETA: 1:17 - loss: 0.0247 - accuracy: 0.99 - ETA: 1:08 - loss: 0.0237 - accuracy: 0.99 - ETA: 59s - loss: 0.0254 - accuracy: 0.9941 - ETA: 50s - loss: 0.0247 - accuracy: 0.994 - ETA: 41s - loss: 0.0238 - accuracy: 0.994 - ETA: 32s - loss: 0.0231 - accuracy: 0.995 - ETA: 23s - loss: 0.0225 - accuracy: 0.995 - ETA: 14s - loss: 0.0346 - accuracy: 0.992 - ETA: 5s - loss: 0.0342 - accuracy: 0.992 - 218s 302ms/step - loss: 0.0338 - accuracy: 0.9931 - val_loss: 1.2860 - val_accuracy: 0.5746\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.57459\n",
      "Epoch 18/20\n",
      "724/724 [==============================] - ETA: 3:18 - loss: 0.0256 - accuracy: 1.00 - ETA: 3:07 - loss: 0.0303 - accuracy: 1.00 - ETA: 2:56 - loss: 0.0234 - accuracy: 1.00 - ETA: 2:47 - loss: 0.0187 - accuracy: 1.00 - ETA: 2:38 - loss: 0.0182 - accuracy: 1.00 - ETA: 2:29 - loss: 0.0166 - accuracy: 1.00 - ETA: 2:22 - loss: 0.0168 - accuracy: 1.00 - ETA: 2:12 - loss: 0.0187 - accuracy: 0.99 - ETA: 2:03 - loss: 0.0171 - accuracy: 0.99 - ETA: 1:54 - loss: 0.0174 - accuracy: 0.99 - ETA: 1:47 - loss: 0.0188 - accuracy: 0.99 - ETA: 1:38 - loss: 0.0191 - accuracy: 0.99 - ETA: 1:29 - loss: 0.0187 - accuracy: 0.99 - ETA: 1:20 - loss: 0.0178 - accuracy: 0.99 - ETA: 1:11 - loss: 0.0169 - accuracy: 0.99 - ETA: 1:01 - loss: 0.0161 - accuracy: 0.99 - ETA: 52s - loss: 0.0158 - accuracy: 0.9963 - ETA: 42s - loss: 0.0154 - accuracy: 0.996 - ETA: 33s - loss: 0.0149 - accuracy: 0.996 - ETA: 24s - loss: 0.0145 - accuracy: 0.996 - ETA: 14s - loss: 0.0178 - accuracy: 0.995 - ETA: 5s - loss: 0.0177 - accuracy: 0.995 - 225s 311ms/step - loss: 0.0173 - accuracy: 0.9959 - val_loss: 1.2963 - val_accuracy: 0.5967\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.57459 to 0.59669, saving model to Model.hdf5\n",
      "Epoch 19/20\n",
      "724/724 [==============================] - ETA: 3:12 - loss: 0.0089 - accuracy: 1.00 - ETA: 3:04 - loss: 0.0059 - accuracy: 1.00 - ETA: 2:55 - loss: 0.0083 - accuracy: 1.00 - ETA: 2:47 - loss: 0.0130 - accuracy: 1.00 - ETA: 2:38 - loss: 0.0132 - accuracy: 1.00 - ETA: 2:28 - loss: 0.0121 - accuracy: 1.00 - ETA: 2:19 - loss: 0.0114 - accuracy: 1.00 - ETA: 2:11 - loss: 0.0104 - accuracy: 1.00 - ETA: 2:02 - loss: 0.0101 - accuracy: 1.00 - ETA: 1:53 - loss: 0.0108 - accuracy: 1.00 - ETA: 1:44 - loss: 0.0102 - accuracy: 1.00 - ETA: 1:35 - loss: 0.0099 - accuracy: 1.00 - ETA: 1:26 - loss: 0.0093 - accuracy: 1.00 - ETA: 1:17 - loss: 0.0154 - accuracy: 0.99 - ETA: 1:08 - loss: 0.0177 - accuracy: 0.99 - ETA: 59s - loss: 0.0167 - accuracy: 0.9961 - ETA: 50s - loss: 0.0161 - accuracy: 0.996 - ETA: 41s - loss: 0.0162 - accuracy: 0.996 - ETA: 32s - loss: 0.0161 - accuracy: 0.996 - ETA: 23s - loss: 0.0218 - accuracy: 0.995 - ETA: 14s - loss: 0.0212 - accuracy: 0.995 - ETA: 5s - loss: 0.0211 - accuracy: 0.995 - 220s 304ms/step - loss: 0.0208 - accuracy: 0.9959 - val_loss: 1.5412 - val_accuracy: 0.4751\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.59669\n",
      "Epoch 20/20\n",
      "724/724 [==============================] - ETA: 3:30 - loss: 0.0092 - accuracy: 1.00 - ETA: 3:16 - loss: 0.0054 - accuracy: 1.00 - ETA: 3:06 - loss: 0.0403 - accuracy: 0.98 - ETA: 2:56 - loss: 0.0350 - accuracy: 0.99 - ETA: 2:49 - loss: 0.0313 - accuracy: 0.99 - ETA: 2:40 - loss: 0.0269 - accuracy: 0.99 - ETA: 2:31 - loss: 0.0243 - accuracy: 0.99 - ETA: 2:21 - loss: 0.0261 - accuracy: 0.99 - ETA: 2:11 - loss: 0.0316 - accuracy: 0.98 - ETA: 2:04 - loss: 0.0299 - accuracy: 0.99 - ETA: 1:57 - loss: 0.0278 - accuracy: 0.99 - ETA: 1:51 - loss: 0.0297 - accuracy: 0.98 - ETA: 1:43 - loss: 0.0291 - accuracy: 0.99 - ETA: 1:33 - loss: 0.0304 - accuracy: 0.98 - ETA: 1:23 - loss: 0.0286 - accuracy: 0.98 - ETA: 1:13 - loss: 0.0271 - accuracy: 0.99 - ETA: 1:02 - loss: 0.0260 - accuracy: 0.99 - ETA: 51s - loss: 0.0247 - accuracy: 0.9913 - ETA: 40s - loss: 0.0239 - accuracy: 0.991 - ETA: 28s - loss: 0.0229 - accuracy: 0.992 - ETA: 17s - loss: 0.0221 - accuracy: 0.992 - ETA: 6s - loss: 0.0213 - accuracy: 0.992 - 264s 365ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 1.4608 - val_accuracy: 0.5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.59669\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),callbacks=[checkpoint]) # Fit for the current dataset\n",
    "model.load_weights(\"Model.hdf5\")\n",
    "answer = model.predict(x_test) # Get prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"Model.hdf5\")\n",
    "# testCase = tokenizer.texts_to_sequences((np.array(documents))[[0,1,2,3,4,5,6,7,8,9,10]])\n",
    "# testCase = pad_sequences(testCase, padding='post', maxlen=maxlen)\n",
    "# answer = model.predict(testCase) # Get prediction output\n",
    "# print(encoder.inverse_transform(answer.argmax(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11, 181]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f0c86615d0e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision score micro: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"micro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Print micro precision for final class value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision score macro: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Print macro precision for final class value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1570\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1415\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                          str(average_options))\n\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [11, 181]"
     ]
    }
   ],
   "source": [
    "print(\"Precision score micro: \",precision_score(answer.argmax(axis=-1),y_test.argmax(axis=-1),average=\"micro\")) # Print micro precision for final class value\n",
    "print(\"Precision score macro: \",precision_score(answer.argmax(axis=-1),y_test.argmax(axis=-1),average=\"macro\")) # Print macro precision for final class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall score micro : \",recall_score(answer.argmax(axis=-1),y_test.argmax(axis=-1),average=\"micro\")) # Print micro recall for final class value\n",
    "print(\"Recall score macro : \",recall_score(answer.argmax(axis=-1),y_test.argmax(axis=-1),average=\"macro\")) # Print macro recall for final class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score micro : \",f1_score(answer.argmax(axis=-1),y_test.argmax(axis=-1),average=\"micro\")) # Print micro f1_score for final class value\n",
    "print(\"F1 score macro : \",f1_score(answer.argmax(axis=-1),y_test.argmax(axis=-1),average=\"macro\")) # Print macro f1_score for final class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(answer.argmax(axis=-1),y_test.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(20)),y=history.history[\"val_accuracy\"],mode=\"markers+lines\"))\n",
    "\n",
    "# plt.plot(list(range(20)),history.history[\"val_accuracy\"],label=\"validation_accuracy\")\n",
    "# plt.plot(list(range(20)),history.history[\"accuracy\"],label=\"Training accuracy\")\n",
    "# plt.plot(list(range(20)),history.history[\"loss\"],label=\"Training loss\")\n",
    "# plt.plot(list(range(20)),history.history[\"val_loss\"],label=\"Training loss\")\n",
    "# plt.legend()\n",
    "# plt.title(\"epoch vs accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code to print 10 fold cross validation \n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "finalAccuracy = []\n",
    "finalPrecisionMacro = []\n",
    "finalPrecisionMicro = []\n",
    "finalRecallMacro = []\n",
    "finalRecallMicro = []\n",
    "finalF1ScoreMacro = []\n",
    "finalF1ScoreMicro = []\n",
    "count = 0\n",
    "for train,test in kfold.split(documents,oldy):\n",
    "    encodedY = encoder.transform(oldy) # Transform label \n",
    "    y = np_utils.to_categorical(encodedY) # Get categorial label here 1X3 as 3 labels given\n",
    "    tokenizer = Tokenizer(num_words=max_features) # Create tokenizer \n",
    "    tokenizer.fit_on_texts(np.array(documents)[train]) # Fit train data to tokenizer\n",
    "    x_train = tokenizer.texts_to_sequences(np.array(documents)[train]) # train_x data with tokeinzer output\n",
    "    x_test = tokenizer.texts_to_sequences(np.array(documents)[test]) # test_x data with tokeinzer output\n",
    "    x_train = pad_sequences(x_train, padding='post', maxlen=maxlen) # Pad sequence so that length of document remains same -- remove extra data\n",
    "    x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)  # Pad sequence so that length of document remains same -- remove extra data\n",
    "    checkpoint = ModelCheckpoint(\"Model.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    model = Model(inputs=inputLayer, outputs=activationSoftmax) # Add input and output layer to create the network\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) # Compute with adamic adar and categorial cross entropy\n",
    "    model.fit(x_train, y[train],batch_size=batch_size,epochs=10,validation_data=(x_test, y[test]),callbacks=[checkpoint]) # Fit for the current dataset\n",
    "    model.load_weights(\"Model.hdf5\")\n",
    "    answer = model.predict(x_test) \n",
    "\n",
    "    count+=1\n",
    "    finalAccuracy += [accuracy_score(answer.argmax(axis=-1),y[test].argmax(axis=-1))]\n",
    "    finalPrecisionMacro += [precision_score(answer.argmax(axis=-1),y[test].argmax(axis=-1),average=\"macro\")]\n",
    "    finalPrecisionMicro += [precision_score(answer.argmax(axis=-1),y[test].argmax(axis=-1),average=\"micro\")]\n",
    "    finalRecallMicro += [recall_score(answer.argmax(axis=-1),y[test].argmax(axis=-1),average=\"micro\")]\n",
    "    finalRecallMacro += [recall_score(answer.argmax(axis=-1),y[test].argmax(axis=-1),average=\"macro\")]\n",
    "    finalF1ScoreMicro += [f1_score(answer.argmax(axis=-1),y[test].argmax(axis=-1),average=\"micro\")]\n",
    "    finalF1ScoreMacro += [f1_score(answer.argmax(axis=-1),y[test].argmax(axis=-1),average=\"macro\")]\n",
    "\n",
    "    print(sum(finalAccuracy)/count,sum(finalPrecisionMacro)/count,sum(finalPrecisionMicro)/count,sum(finalRecallMicro)/count,sum(finalRecallMacro)/count,sum(finalF1ScoreMicro)/count,sum(finalF1ScoreMacro)/count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
